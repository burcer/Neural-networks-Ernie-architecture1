{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/burc/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "0.0\n",
      "255.0\n",
      "(50000, 28, 28)\n",
      "42.5\n",
      "212.5\n",
      "X_val:  (10000, 28, 28)\n",
      "X_train:  (50000, 28, 28)\n",
      "X_test:  (10000, 28, 28)\n",
      "y_val:  (10000,)\n",
      "y_train:  (50000,)\n",
      "y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "##Sukru Burc Eryilmaz\n",
    "##Trains a 4-layer FC neural network with several options in terms of activation quantization, noisy ensembles, \n",
    "##noisy ensembles, and splitting the FC layers into a number of (2-8) locally connected independent blocks (towers)\n",
    "##This particular code uses MNIST dataset, but CIFAR10 is also available by changing the data input size in model \n",
    "##and get data command to CIFAR10 case.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "##This particular code implements splitted FC layers (FC layers splitted to independent 2-8 locally connected layers). If\n",
    "##full FC layer is desired, use 'from nn.classifiers.fc_net_nosplit3 import *' instead of 'from nn.classifiers.fc_net_split3_3 import *'\n",
    "from nn.classifiers.fc_net_split3_3 import *\n",
    "##\n",
    "from nn.data_utils import get_CIFAR10_data\n",
    "from nn.data_utils_mnist import *\n",
    "from nn.solver import Solver\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "##load MNIST data.  xxx_trinary=1 indicates that the corresponding portion of data (i.e. test data for test_trinary=1)\n",
    "##is quantized with no_of_levels levels. Here all data is quantized to 3 levels (ternary). You can quantize data to any\n",
    "##number of levels, provided it is a positive integer larger than 1.\n",
    "\n",
    "data = get_MNIST_data(test_trinary=1,train_trinary=1,val_trinary=1,noise_amplitude=0.0,no_of_levels=3)\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=data['X_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/layers.py:237: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.000553, std=0.015509, regu=0.000392 ,dec= 0.996765 train accuracy is : 0.947000 val acc:  0.948500\n",
      " no decay lr=0.000553, std=0.015509,regu=0.000392 train accuracy is : 0.950000 val acc:  0.947800\n",
      "lr=0.000121, std=0.015343, regu=0.001327 ,dec= 0.993828 train accuracy is : 0.834000 val acc:  0.851600\n",
      " no decay lr=0.000121, std=0.015343,regu=0.001327 train accuracy is : 0.853000 val acc:  0.839100\n",
      "lr=0.002052, std=0.001064, regu=0.003139 ,dec= 0.971469 train accuracy is : 0.949000 val acc:  0.955700\n",
      " no decay lr=0.002052, std=0.001064,regu=0.003139 train accuracy is : 0.913000 val acc:  0.914800\n",
      "lr=0.004608, std=0.012343, regu=0.001837 ,dec= 0.989575 train accuracy is : 0.873000 val acc:  0.891500\n",
      " no decay lr=0.004608, std=0.012343,regu=0.001837 train accuracy is : 0.854000 val acc:  0.883100\n",
      "lr=0.001246, std=0.007923, regu=0.003754 ,dec= 0.985792 train accuracy is : 0.969000 val acc:  0.959800\n",
      " no decay lr=0.001246, std=0.007923,regu=0.003754 train accuracy is : 0.955000 val acc:  0.948400\n",
      "lr=0.000394, std=0.085658, regu=0.000296 ,dec= 0.979531 train accuracy is : 0.959000 val acc:  0.947500\n",
      " no decay lr=0.000394, std=0.085658,regu=0.000296 train accuracy is : 0.946000 val acc:  0.946800\n",
      "lr=0.000499, std=0.003124, regu=0.044084 ,dec= 0.975921 train accuracy is : 0.871000 val acc:  0.874700\n",
      " no decay lr=0.000499, std=0.003124,regu=0.044084 train accuracy is : 0.929000 val acc:  0.928300\n",
      "lr=0.000115, std=0.001162, regu=0.018749 ,dec= 0.985175 train accuracy is : 0.633000 val acc:  0.668400\n",
      " no decay lr=0.000115, std=0.001162,regu=0.018749 train accuracy is : 0.703000 val acc:  0.716300\n",
      "lr=0.006597, std=0.012303, regu=0.002664 ,dec= 0.969377 train accuracy is : 0.898000 val acc:  0.911600\n",
      " no decay lr=0.006597, std=0.012303,regu=0.002664 train accuracy is : 0.817000 val acc:  0.839500\n",
      "lr=0.000184, std=0.001214, regu=0.007081 ,dec= 0.991819 train accuracy is : 0.824000 val acc:  0.827700\n",
      " no decay lr=0.000184, std=0.001214,regu=0.007081 train accuracy is : 0.874000 val acc:  0.851500\n",
      "lr=0.005300, std=0.050017, regu=0.002129 ,dec= 0.987698 train accuracy is : 0.886000 val acc:  0.903600\n",
      " no decay lr=0.005300, std=0.050017,regu=0.002129 train accuracy is : 0.850000 val acc:  0.844900\n",
      "lr=0.001054, std=0.002147, regu=0.004231 ,dec= 0.989078 train accuracy is : 0.956000 val acc:  0.954700\n",
      " no decay lr=0.001054, std=0.002147,regu=0.004231 train accuracy is : 0.950000 val acc:  0.954100\n",
      "lr=0.005015, std=0.051761, regu=0.006569 ,dec= 0.999802 train accuracy is : 0.908000 val acc:  0.915900\n",
      " no decay lr=0.005015, std=0.051761,regu=0.006569 train accuracy is : 0.897000 val acc:  0.907600\n",
      "lr=0.002959, std=0.003454, regu=0.019985 ,dec= 0.989746 train accuracy is : 0.941000 val acc:  0.947300\n",
      " no decay lr=0.002959, std=0.003454,regu=0.019985 train accuracy is : 0.952000 val acc:  0.942000\n",
      "lr=0.001779, std=0.065360, regu=0.015879 ,dec= 0.982451 train accuracy is : 0.961000 val acc:  0.958600\n",
      " no decay lr=0.001779, std=0.065360,regu=0.015879 train accuracy is : 0.944000 val acc:  0.952100\n",
      "lr=0.000316, std=0.001056, regu=0.024788 ,dec= 0.973074 train accuracy is : 0.845000 val acc:  0.846400\n",
      " no decay lr=0.000316, std=0.001056,regu=0.024788 train accuracy is : 0.871000 val acc:  0.865400\n",
      "lr=0.001129, std=0.037994, regu=0.001286 ,dec= 0.996981 train accuracy is : 0.953000 val acc:  0.948600\n",
      " no decay lr=0.001129, std=0.037994,regu=0.001286 train accuracy is : 0.934000 val acc:  0.940400\n",
      "lr=0.000654, std=0.003563, regu=0.000503 ,dec= 0.964588 train accuracy is : 0.927000 val acc:  0.930700\n",
      " no decay lr=0.000654, std=0.003563,regu=0.000503 train accuracy is : 0.950000 val acc:  0.939300\n",
      "lr=0.000963, std=0.001660, regu=0.016300 ,dec= 0.996739 train accuracy is : 0.945000 val acc:  0.950400\n",
      " no decay lr=0.000963, std=0.001660,regu=0.016300 train accuracy is : 0.947000 val acc:  0.951000\n",
      "lr=0.004041, std=0.041798, regu=0.003542 ,dec= 0.954575 train accuracy is : 0.963000 val acc:  0.950600\n",
      " no decay lr=0.004041, std=0.041798,regu=0.003542 train accuracy is : 0.900000 val acc:  0.908500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2652a64c1967>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m          )\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'lr=%f, std=%f, regu=%f ,dec= %f train accuracy is : %f val acc:  %f'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_acc_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_acc_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/burc/assignment2/cs231n/solver.pyc\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m       \u001b[1;31m# Maybe print training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/burc/assignment2/cs231n/solver.pyc\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;31m# Compute loss and gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/burc/assignment2/cs231n/classifiers/fc_net_split3_3.pyc\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, X, y, noise, noise2, test, parallel_samples_output)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;31m#dhidden, dW3, db3 = affine_backward(dscores, score_cache)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m       \u001b[0mdx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maffine_tanh_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_cache33\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#change to affine_sigmoid later\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/burc/assignment2/cs231n/layer_utils.pyc\u001b[0m in \u001b[0;36maffine_tanh_backward\u001b[1;34m(dout, cache)\u001b[0m\n\u001b[0;32m     55\u001b[0m   \u001b[0mfc_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[0mda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtanh_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m   \u001b[0mdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maffine_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfc_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/burc/assignment2/cs231n/layers.pyc\u001b[0m in \u001b[0;36maffine_backward\u001b[1;34m(dout, cache)\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m   \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m   \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_reshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m   \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;31m#############################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#hyperparameter search\n",
    "##In FourLayerNet class:\n",
    "##weight_scale: defines the standard deviation for initialization. \n",
    "##reg argument: weight decay coefficient\n",
    "##scores_activation: defines the activation function used for top score layer. Options are linear, relu, tanh, sigmoid,\n",
    "##noisy tanh quantization with the number of levels equal to 'levels' argument\n",
    "##activation: same as above, but for hidden layers\n",
    "##external_load: determines if the bottom N-1 layers are externally loaded from a pretrained network or all the N layers are\n",
    "##trained. When it is 1, weight_scale is ignored\n",
    "##levels: number of quantization levels in activation\n",
    "##see the class definitions for FourLayerNet and Solver to understand the details. \n",
    "for i in range(100):\n",
    "\n",
    "    regu=10**np.random.uniform(-4,-1)\n",
    "    learning_rate = 10**np.random.uniform(-4,-2)\n",
    "    weight_scale = 10**np.random.uniform(-3,-1)\n",
    "    dec = np.random.uniform(0.95,1)\n",
    "\n",
    "    model = FourLayerNet(input_dim=28*28, \n",
    "              weight_scale=weight_scale, reg=regu, activation=3, scores_activation=3,external_load=1,levels=3)\n",
    "    solver = Solver(model,data,\n",
    "                print_every=10, num_epochs=60, batch_size=100,\n",
    "                update_rule='sgd_momentum',lr_decay=dec,verbose = False,\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate\n",
    "                }\n",
    "         )\n",
    "    \n",
    "    solver.train()\n",
    "\n",
    "    print 'lr=%f, std=%f, regu=%f ,dec= %f train accuracy is : %f val acc:  %f' %(learning_rate, weight_scale, regu, dec,solver.train_acc_history[-1], solver.val_acc_history[-1])\n",
    " \n",
    "\n",
    "    model = FourLayerNet(input_dim=28*28,  \n",
    "              weight_scale=weight_scale, reg=regu,activation=3, scores_activation=3,external_load=1,levels=3)\n",
    "    solver = Solver(model,data,\n",
    "                print_every=4000, num_epochs=60, batch_size=100, \n",
    "                update_rule='sgd_momentum',lr_decay=1.0,verbose =False, \n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate\n",
    "                }\n",
    "         )\n",
    "    solver.train()\n",
    "    print ' no decay lr=%f, std=%f,regu=%f train accuracy is : %f val acc:  %f' %(learning_rate, weight_scale,regu, solver.train_acc_history[-1], solver.val_acc_history[-1])\n",
    " \n",
    "\n",
    "\n",
    "#solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.001246, std=0.007923, regu=0.003754 ,dec= 0.985792 train accuracy is : 0.962000 val acc:  0.958200\n"
     ]
    }
   ],
   "source": [
    "#optimum onfiguration found: lr=0.001246, std=0.007923, regu=0.003754 ,dec= 0.985792 train accuracy is : 0.969000 val acc:  0.959800\n",
    "#retrain with optimum configuration\n",
    "regu=0.003754\n",
    "learning_rate = 0.001246\n",
    "weight_scale = 0.007923\n",
    "dec = 0.985792 \n",
    "\n",
    "model = FourLayerNet(input_dim=28*28, ##hidden_dim=256,\n",
    "              weight_scale=weight_scale, reg=regu, activation=3, scores_activation=3,external_load=1,levels=3)\n",
    "solver = Solver(model,data,\n",
    "                print_every=10, num_epochs=60, batch_size=100,\n",
    "                update_rule='sgd_momentum',lr_decay=dec,verbose = False,\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate\n",
    "                }\n",
    "         )\n",
    "    \n",
    "solver.train()\n",
    "\n",
    "print 'lr=%f, std=%f, regu=%f ,dec= %f train accuracy is : %f val acc:  %f' %(learning_rate, weight_scale, regu, dec,solver.train_acc_history[-1], solver.val_acc_history[-1])\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy:  0.9582\n",
      "Test set accuracy:  0.9554\n",
      "Noisy test set accuracy:  0.7882\n",
      "0.554815\n",
      "0.58738\n",
      "0.603055\n",
      "0.60989\n",
      "0.6162\n",
      "0.61703\n",
      "0.61644\n"
     ]
    }
   ],
   "source": [
    "#this piece of code experiments with classification accuracy by quantizing activations, \n",
    "##introducing noise, using noisy ensembles at the class layer to mitigate degradation due to quantization\n",
    "##noise =1 indicates noise is introduced during quantizing activations. noise2=1 indicates that noise is introduced \n",
    "##at the softmax layer during quantization, and a number of ensembles are averaged with this noise at the softmax layer \n",
    "##to find the final class scores. the number of ensembles averaged at softmax layer are given by the argument \n",
    "##parallel_samples_output. Note in the results that more ensembles result in better accuracy.\n",
    "\n",
    "y_test_pred = np.argmax(model.loss(data['X_test'],noise=0,test=1), axis=1)\n",
    "y_val_pred = np.argmax(model.loss(data['X_val']), axis=1)\n",
    "print 'Validation set accuracy: ', (y_val_pred == data['y_val']).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "y_test_pred = np.argmax(model.loss(data['X_test'],noise=1,test=1), axis=1)\n",
    "print 'Noisy test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "\n",
    "par=1\n",
    "aa=0.0\n",
    "for i in range(0,20):\n",
    "  y_test_pred = np.argmax(model.loss(data['X_test'],noise=1,test=1,noise2=1, parallel_samples_output=par), axis=1)\n",
    "  #print 'Noisy test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "  aa=aa+ (y_test_pred == data['y_test']).mean()\n",
    "print aa/20\n",
    "\n",
    "par=2\n",
    "aa=0.0\n",
    "for i in range(0,20):\n",
    "  y_test_pred = np.argmax(model.loss(data['X_test'],noise=1,test=1,noise2=1, parallel_samples_output=par), axis=1)\n",
    "  #print 'Noisy test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "  aa=aa+ (y_test_pred == data['y_test']).mean()\n",
    "print aa/20\n",
    "\n",
    "\n",
    "par=3\n",
    "aa=0.0\n",
    "for i in range(0,20):\n",
    "  y_test_pred = np.argmax(model.loss(data['X_test'],noise=1,test=1,noise2=1, parallel_samples_output=par), axis=1)\n",
    "  #print 'Noisy test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "  aa=aa+ (y_test_pred == data['y_test']).mean()\n",
    "print aa/20\n",
    "\n",
    "par=4\n",
    "aa=0.0\n",
    "for i in range(0,20):\n",
    "  y_test_pred = np.argmax(model.loss(data['X_test'],noise=1,test=1,noise2=1, parallel_samples_output=par), axis=1)\n",
    "  #print 'Noisy test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "  aa=aa+ (y_test_pred == data['y_test']).mean()\n",
    "print aa/20\n",
    "\n",
    "par=8\n",
    "aa=0.0\n",
    "for i in range(0,20):\n",
    "  y_test_pred = np.argmax(model.loss(data['X_test'],noise=1,test=1,noise2=1, parallel_samples_output=par), axis=1)\n",
    "  #print 'Noisy test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "  aa=aa+ (y_test_pred == data['y_test']).mean()\n",
    "print aa/20\n",
    "\n",
    "par=16\n",
    "aa=0.0\n",
    "for i in range(0,20):\n",
    "  y_test_pred = np.argmax(model.loss(data['X_test'],noise=1,test=1,noise2=1, parallel_samples_output=par), axis=1)\n",
    "  #print 'Noisy test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "  aa=aa+ (y_test_pred == data['y_test']).mean()\n",
    "print aa/20\n",
    "\n",
    "par=24\n",
    "aa=0.0\n",
    "for i in range(0,20):\n",
    "  y_test_pred = np.argmax(model.loss(data['X_test'],noise=1,test=1,noise2=1, parallel_samples_output=par), axis=1)\n",
    "  #print 'Noisy test set accuracy: ', (y_test_pred == data['y_test']).mean()\n",
    "  aa=aa+ (y_test_pred == data['y_test']).mean()\n",
    "print aa/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##toggle the comment for the following piece of code to save the model variables.\n",
    "\n",
    "#np.save('W1_1ext784-256-256-10-3level', model.params['W1_1'])\n",
    "#np.save('b1_1ext784-256-256-10-3level', model.params['b1_1'])\n",
    "#np.save('W1_2ext784-256-256-10-3level', model.params['W1_2'])\n",
    "#np.save('b1_2ext784-256-256-10-3level', model.params['b1_2'])\n",
    "#np.save('W1_3ext784-256-256-10-3level', model.params['W1_3'])\n",
    "#np.save('b1_3ext784-256-256-10-3level', model.params['b1_3'])\n",
    "#np.save('W1_4ext784-256-256-10-3level', model.params['W1_4'])\n",
    "#np.save('b1_4ext784-256-256-10-3level', model.params['b1_4'])\n",
    "#np.save('W2ext784-256-256-10-3level', model.params['W2'])\n",
    "#np.save('b2ext784-256-256-10-3level', model.params['b2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
